{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import types\n",
    "import time\n",
    "import re\n",
    "import queue\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read the inverted frequency index from file\n",
    "def ReadIndex(filename):\n",
    "    path = 'Indexer_Aneesh/'\n",
    "    file = open(path+filename,'r')\n",
    "    inv_index = {}\n",
    "    doc = file.readlines()\n",
    "    for line in doc:\n",
    "        line = line.split(':')\n",
    "        word = line[0].strip(' ')\n",
    "        line = line[1].strip(' ').split(', [')\n",
    "        clean_line = []\n",
    "        for term in line:\n",
    "            term = term.strip(',( \\n)[]').split(', ')\n",
    "            clean_line.append(term) \n",
    "        inv_index[word] = clean_line\n",
    "    return inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get ocurrence based partial index\n",
    "def getPartIndex(inv_index):\n",
    "    temp = inv_index.copy()\n",
    "    for word in inv_index:\n",
    "        temp[word] = []\n",
    "        for record in inv_index[word]:\n",
    "            temp[word].append(record[0])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Returns the intersect of the terms of inv_lists\n",
    "def Intersect(inv_lists):\n",
    "    sect_terms = inv_lists[0]\n",
    "    for term in inv_lists:\n",
    "        sect_terms = set(sect_terms).intersection(term)\n",
    "    return sect_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates the support of every itemset\n",
    "def GetSup(itemset,par_index,d_sup_vals,num_docs):\n",
    "    if tuple(itemset) in d_sup_vals:\n",
    "        return d_sup_vals[tuple(itemset)]\n",
    "    inv_lists = []\n",
    "    for term in itemset:\n",
    "        inv_lists.append(par_index[term])\n",
    "    else:\n",
    "        sup = len(Intersect(inv_lists))/num_docs\n",
    "        d_sup_vals[tuple(itemset)] = sup\n",
    "    return sup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determine if two sets need to be joined\n",
    "def NeedJoin(set1,set2):\n",
    "    for i in range(0,len(set1)-1):\n",
    "        if set1[i] != set2[i]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Joins two sets\n",
    "def Join(set1,set2):\n",
    "    set3 = list(set1)\n",
    "    set3.append(set2[-1])\n",
    "    return set3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creates an empty list to fill in with frequent itemsets\n",
    "def getFreqSets(num_terms):\n",
    "    freq_itemsets = []\n",
    "    for i in range(0,num_terms+1):\n",
    "        temp = []\n",
    "        freq_itemsets.append(temp)\n",
    "    return freq_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Saves the rules to a text file\n",
    "def SaveRules(rules,num_rules):\n",
    "    file = open('Association Rules.txt','w')\n",
    "    file.write('Number of rules generated: '+str(num_rules)+'\\n')\n",
    "    for rule in rules:\n",
    "        file.write(str(rule[0])+' => '+str(rule[1])+'; support='+str(rule[2])+', confidence='+str(rule[3])+'\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Count the number of crawled documents\n",
    "def getNumDocs(par_index,path):\n",
    "    num_docs = 0\n",
    "    for filename in glob.glob(os.path.join(path,'*.html')):\n",
    "        num_docs += 1\n",
    "    return num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the related terms to be searched\n",
    "def getRelTerms():\n",
    "    return ['india','cricket','pakistan','additional','ball','africa','american','test','pitch',\\\n",
    "            'world','wicket','accepted','tendulkar','player','bowler','australia','bcci','defeats','loss',\\\n",
    "           'win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the top ten association rules:\n",
    "def getTop(rules):\n",
    "    top_rules = sorted(rules, key=lambda x:x[2],reverse=True)\n",
    "    top_rules = top_rules[0:10]\n",
    "    top_rules = sorted(top_rules, key=lambda x:x[3],reverse=True)\n",
    "    return top_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the report for the assignment:\n",
    "def createReport(rules,num_rules,min_sup,min_conf):\n",
    "    top_rules = getTop(rules)\n",
    "    file = open('Association_Report.txt','w')\n",
    "    file.write('1) This program was developed in Python, and generates a set of Association rules '+'\\n')\n",
    "    file.write('from a provided index file, so as to find useful patterns within the text.'+'\\n'+'\\n')\n",
    "    file.write('2) With a minimum support of '+str(min_sup)+' and a minimun confidence of '+str(min_conf)+'\\n')\n",
    "    file.write('the algorithm generated '+str(num_rules)+' Association Rules, of which the top ten are:'+'\\n' +'\\n')\n",
    "    for rule in top_rules:\n",
    "        file.write(str(rule[0])+' => '+str(rule[1])+'; support='+str(rule[2])+', confidence='+str(rule[3])+'\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('3) Overall, decreasing the minimun support or the minimum confidence gave me more association'+'\\n')\n",
    "    file.write('rules, but of a lesser quality. However, having the minimum thresholds be too high led to few rules'+'\\n')\n",
    "    file.write('and at the same time they were so obvious as to be useless. In the end I chose the current'+'\\n')\n",
    "    file.write('thresholds because they appear the ideal combination of quantity and quality.')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rel_terms = getRelTerms()\n",
    "min_sup = 0.25\n",
    "min_conf = 0.7\n",
    "inv_index = ReadIndex('Frequency Index_Aneesh.txt')\n",
    "par_index = getPartIndex(inv_index)\n",
    "d_sup_vals = {}\n",
    "path = 'Crawled_Pages'+'/'\n",
    "num_terms = len(rel_terms)\n",
    "num_docs = getNumDocs(par_index,path)\n",
    "freq_itemsets = getFreqSets(num_terms)\n",
    "#Collection of frequent 1-itemsets\n",
    "for term in rel_terms:\n",
    "    itemset = [term]\n",
    "    if GetSup(itemset,par_index,d_sup_vals,num_docs)>min_sup:\n",
    "        freq_itemsets[1].append(itemset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find frequent 2 to K-itemsets\n",
    "for k in range(2,num_terms+1):\n",
    "    prev_itemsets = freq_itemsets[k-1]\n",
    "    n = len(prev_itemsets)\n",
    "    for i in range(1,n-1):\n",
    "        for j in range(i+1,n):\n",
    "            set1 = prev_itemsets[i]\n",
    "            set2 = prev_itemsets[j]\n",
    "            if NeedJoin(set1,set2):\n",
    "                set3 = Join(set1,set2)\n",
    "                if GetSup(set3,par_index,d_sup_vals,num_docs)>min_sup:\n",
    "                    freq_itemsets[k].append(set3)\n",
    "    if len(freq_itemsets) == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate the Association Rules\n",
    "rules = []\n",
    "for k in range(2,num_terms+1):\n",
    "    if not freq_itemsets:\n",
    "        break\n",
    "    for itemset in freq_itemsets[k]:\n",
    "        for i in range(0,k):\n",
    "            itemset2 = list(itemset)\n",
    "            itemset2.remove(itemset2[i])\n",
    "            sup1 = GetSup(itemset2,par_index,d_sup_vals,num_docs)\n",
    "            sup2 = GetSup(itemset,par_index,d_sup_vals,num_docs)\n",
    "            conf = sup2/sup1\n",
    "            if conf>min_conf:\n",
    "                rule = []\n",
    "                rule.append(itemset2)\n",
    "                rule.append(itemset[i])\n",
    "                rule.append(sup2)\n",
    "                rule.append(conf)\n",
    "                rules.append(rule)\n",
    "num_rules = len(rules)\n",
    "SaveRules(rules,num_rules)\n",
    "createReport(rules,num_rules,min_sup,min_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
