{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to tokenizers...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords','tokenizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import types\n",
    "import time\n",
    "import re\n",
    "import queue\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Strip the words in doc of any characters that are not alphabets\n",
    "def makeVal(word):\n",
    "    word = word.strip(string.punctuation)\n",
    "    regex=re.compile('[^a-zA-Z]')\n",
    "    regex.sub('',word)\n",
    "    word = word.split()\n",
    "    out_word=''\n",
    "    for letter in word:\n",
    "        if letter.isalpha():\n",
    "            out_word = out_word+letter\n",
    "    return out_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#return list of words in the file\n",
    "def getPosTokens(filename):\n",
    "    file = open(filename,'r')\n",
    "    html = file.read()\n",
    "    doc = BeautifulSoup(html,'lxml').get_text()\n",
    "    #doc = ''\n",
    "    #for para in soup.find_all('p'):\n",
    "        #para = para.find_all(text=True)\n",
    "        #text = ''\n",
    "        #for line in para:\n",
    "            #text = text+line+' '\n",
    "        #doc = doc+text+' '\n",
    "    doc = doc.lower()\n",
    "    doc = doc.split(' ')\n",
    "    tokens = []\n",
    "    for word in doc:\n",
    "        word = makeVal(word)\n",
    "        if len(word)>1 and word != 'html':\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#return list of words in the file, remove stop words\n",
    "def getFreqTokens(filename):\n",
    "    file = open(filename,'r')\n",
    "    html = file.read()\n",
    "    doc = BeautifulSoup(html,'lxml').get_text()\n",
    "    #doc = ''\n",
    "    #for para in soup.find_all('p'):\n",
    "        #para = para.find_all(text=True)\n",
    "        #text = ''\n",
    "        #for line in para:\n",
    "           #text = text+line+' '\n",
    "        #doc = doc+text+' '\n",
    "    doc = doc.lower()\n",
    "    doc = doc.split(' ')\n",
    "    tokens = []\n",
    "    for word in doc:\n",
    "        word = makeVal(word)\n",
    "        if word not in stopwords.words('english') and len(word)>1 and word != 'html':\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFilePath(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Alphabetically sort the inverted index\n",
    "def getSortedList(inv_index):\n",
    "    sorted_index = []\n",
    "    for word in inv_index:\n",
    "        value = str(tuple(inv_index[word]))\n",
    "        sorted_index.append(word + \" : \" + value)\n",
    "    sorted_index = sorted(sorted_index)\n",
    "    return sorted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create the list of unique words in the index\n",
    "def getUniqueList(inv_index):\n",
    "    num_uni_words = len(inv_index)\n",
    "    sorted_index = []\n",
    "    makeFilePath('Indexer_Aneesh/')\n",
    "    path = 'Indexer_Aneesh/Vocabulary_Aneesh.txt'\n",
    "    file = open(path,'w+')\n",
    "    file.write('Number of unique words: '+str(num_uni_words)+'\\n')\n",
    "    for word in inv_index:\n",
    "        sorted_index.append(word)\n",
    "    sorted_index = sorted(sorted_index)\n",
    "    for entry in sorted_index:\n",
    "        file.write(entry + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Writes the index to file\n",
    "def getInvertedIndex(inv_index,name):\n",
    "    makeFilePath('Indexer_Aneesh/')\n",
    "    path = 'Indexer_Aneesh/' + name + '.txt'\n",
    "    file = open(path,'w+')\n",
    "    sorted_index = getSortedList(inv_index)\n",
    "    for entry in sorted_index:\n",
    "        file.write(entry + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creates the frequency based index\n",
    "def freqIndex(folder):\n",
    "    #Create the index storage dictionary\n",
    "    inv_index = {}\n",
    "    doc_num = 0\n",
    "    #Read all the files in the crawled pages folder\n",
    "    path = folder + '/'\n",
    "    for filename in glob.glob(os.path.join(path,'*.html')):\n",
    "        doc_num += 1\n",
    "        #Create temp dictionary\n",
    "        d_freq = {}\n",
    "        #Convert the file to a list of tokens, or words\n",
    "        tokens = getFreqTokens(filename)\n",
    "        for word in tokens:\n",
    "            if word not in d_freq:\n",
    "                #add the word to the dictionary as a key, with value 0\n",
    "                d_freq[word]=1\n",
    "            else:\n",
    "                #increment the value of the word in dictionary by 1\n",
    "                d_freq[word]+=1\n",
    "        #write the values of d_temp to inv_index\n",
    "        for word in d_freq:\n",
    "            value = [doc_num,d_freq[word]]\n",
    "            if word not in inv_index:\n",
    "                inv_index[word] = []\n",
    "                inv_index[word].append(value)\n",
    "            else:\n",
    "                inv_index[word].append(value)\n",
    "    #Write the inverted index to file\n",
    "    getInvertedIndex(inv_index,'Frequency Index_Aneesh')\n",
    "    getUniqueList(inv_index)\n",
    "    print('Frequency index created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creates the index report\n",
    "def createReport():\n",
    "    makeFilePath('Indexer_Aneesh/')\n",
    "    path = 'Indexer_Aneesh/' + 'Report_Aneesh' + '.txt'\n",
    "    file = open(path,'w+')\n",
    "    file.write('This program works by reading the crawled web pages, '+'\\n')\n",
    "    file.write('parsing the text using BeautifulSoup package, '+'\\n')\n",
    "    file.write('and tokenising it, creating a list of words. '+'\\n')\n",
    "    file.write('It then removes the stopwords using the nltk package. '+'\\n')\n",
    "    file.write('Afterword, it creates an index file of all the words '+'\\n')\n",
    "    file.write('and their frequencies. It also creates an index with '+'\\n')\n",
    "    file.write('the positions of the words within the documents.')\n",
    "    file.close()\n",
    "    print('Wrote the report!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creates the position based index\n",
    "def posIndex(folder):\n",
    "    #Create the index storage dictionary\n",
    "    inv_index = {}\n",
    "    doc_num = 0\n",
    "    #Read all the files in the crawled pages folder\n",
    "    path = folder + '/'\n",
    "    for filename in glob.glob(os.path.join(path,'*.html')):\n",
    "        doc_num += 1\n",
    "        #Convert the file to a list of tokens, or words\n",
    "        tokens = getPosTokens(filename)\n",
    "        #Get the postion of the word in the file\n",
    "        word_pos = 0\n",
    "        for word in tokens:\n",
    "            value = str(tuple([doc_num,word_pos]))\n",
    "            if word not in stopwords.words('english'):\n",
    "                if word not in inv_index:\n",
    "                    #add the word to the dictionary as a key, with first position\n",
    "                    inv_index[word]= []\n",
    "                    inv_index[word].append(value)\n",
    "                else:\n",
    "                    #add the current postion to index\n",
    "                    inv_index[word].append(value)\n",
    "                #increment word position\n",
    "                word_pos += 1\n",
    "            else:\n",
    "                continue\n",
    "                #increment word position\n",
    "                word_pos += 1\n",
    "    #Write the inverted index to file\n",
    "    getInvertedIndex(inv_index,'Position Index_Aneesh')\n",
    "    print ('Position index created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqIndex('Crawled_Pages')\n",
    "posIndex('Crawled_Pages')\n",
    "createReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
