{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to tokenizers...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords','tokenizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import types\n",
    "import time\n",
    "import re\n",
    "import queue\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Strip the words in doc of any characters that are not alphabets\n",
    "def makeVal(word):\n",
    "    word = word.strip(string.punctuation)\n",
    "    regex=re.compile('[^a-zA-Z]')\n",
    "    regex.sub('',word)\n",
    "    word = word.split()\n",
    "    out_word=''\n",
    "    for letter in word:\n",
    "        if letter.isalpha():\n",
    "            out_word = out_word+letter\n",
    "    return out_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getPosToken(file):\n",
    "    #return list of words in the file, remove the duplicates, remove stop words\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#return list of words in the file, don't remove the duplicates, remove stop words\n",
    "def getTokens(filename):\n",
    "    file = open(filename,'r')\n",
    "    html = file.read()\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    doc = ''\n",
    "    for para in soup.find_all('p'):\n",
    "        para = para.find_all(text=True)\n",
    "        text = ''\n",
    "        for line in para:\n",
    "            text = text+line+' '\n",
    "        doc = doc+text+' '\n",
    "    doc = doc.lower()\n",
    "    doc = doc.split(' ')\n",
    "    tokens = []\n",
    "    for word in doc:\n",
    "        word = makeVal(word)\n",
    "        if word not in stopwords.words('english') and len(word)>1 and word != 'html':\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSortedList(inv_index):\n",
    "    sorted_index = []\n",
    "    for word in inv_index:\n",
    "        value = str(tuple(inv_index[word]))\n",
    "        sorted_index.append(word + \" : \" + value)\n",
    "    sorted_index = sorted(sorted_index)\n",
    "    return sorted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getUniqueList(inv_index):\n",
    "    num_uni_words = len(inv_index)\n",
    "    sorted_index = []\n",
    "    for word in inv_index:\n",
    "        value = str(tuple(inv_index[word]))\n",
    "        sorted_index.append(word)\n",
    "    file = open('Unique_Words.txt','w+')\n",
    "    file.write('Number of unique words: '+str(num_uni_words)+'\\n')\n",
    "    for entry in sorted_index:\n",
    "        file.write(entry + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getInvertedIndex(inv_index,name):\n",
    "    file = open(name+'.txt','w+')\n",
    "    sorted_index = getSortedList(inv_index)\n",
    "    for entry in sorted_index:\n",
    "        file.write(entry + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "def freqIndex(folder):\n",
    "    #Create the index storage dictionary\n",
    "    inv_index = {}\n",
    "    doc_num = 0\n",
    "    #Read all the files in the crawled pages folder\n",
    "    path = folder + '/'\n",
    "    for filename in glob.glob(os.path.join(path,'*.html')):\n",
    "        doc_num += 1\n",
    "        #Create temp dictionary\n",
    "        d_freq = {}\n",
    "        #Convert the file to a list of tokens, or words\n",
    "        tokens = getTokens(filename)\n",
    "        for word in tokens:\n",
    "            if word not in d_freq:\n",
    "                #add the word to the dictionary as a key, with value 0\n",
    "                d_freq[word]=1\n",
    "            else:\n",
    "                #increment the value of the word in dictionary by 1\n",
    "                d_freq[word]+=1\n",
    "        #write the values of d_temp to inv_index\n",
    "        for word in d_freq:\n",
    "            value = [doc_num,d_freq[word]]\n",
    "            if word not in inv_index:\n",
    "                inv_index[word] = []\n",
    "                inv_index[word].append(value)\n",
    "            else:\n",
    "                inv_index[word].append(value)\n",
    "    #Write the inverted index to file\n",
    "    getInvertedIndex(inv_index,'Frequency Index')\n",
    "    getUniqueList(inv_index)\n",
    "    print('Frequency index created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position index created!\n"
     ]
    }
   ],
   "source": [
    "def posIndex(folder):\n",
    "    #Create the index storage dictionary\n",
    "    inv_index = {}\n",
    "    doc_num = 0\n",
    "    #Read all the files in the crawled pages folder\n",
    "    path = folder + '/'\n",
    "    for filename in glob.glob(os.path.join(path,'*.html')):\n",
    "        doc_num += 1\n",
    "        #Convert the file to a list of tokens, or words\n",
    "        tokens = getTokens(filename)\n",
    "        #Get the postion of the word in the file\n",
    "        word_pos = 0\n",
    "        for word in tokens:\n",
    "            value = str(tuple([doc_num,word_pos]))\n",
    "            if word not in inv_index:\n",
    "                #add the word to the dictionary as a key, with first position\n",
    "                inv_index[word]= []\n",
    "                inv_index[word].append(value)\n",
    "            else:\n",
    "                #add the current postion to index\n",
    "                inv_index[word].append(value)\n",
    "            #increment word position\n",
    "            word_pos += 1\n",
    "    #Write the inverted index to file\n",
    "    getInvertedIndex(inv_index,'Position Index')\n",
    "    print ('Position index created!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqIndex('Crawled_Pages')\n",
    "posIndex('Crawled_Pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
