{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to tokenizers...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords','tokenizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import types\n",
    "import time\n",
    "import re\n",
    "import queue\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Strip the words in doc of any characters that are not alphabets\n",
    "def makeVal(word):\n",
    "    word = word.strip(string.punctuation)\n",
    "    regex=re.compile('[^a-zA-Z]')\n",
    "    regex.sub('',word)\n",
    "    word = word.split()\n",
    "    out_word=''\n",
    "    for letter in word:\n",
    "        if letter.isalpha():\n",
    "            out_word = out_word+letter\n",
    "    return out_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getPosToken(file):\n",
    "    #return list of words in the file, remove the duplicates, remove stop words\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#return list of words in the file, don't remove the duplicates, remove stop words\n",
    "def getTokens(filename):\n",
    "    file = open(filename,'r')\n",
    "    html = file.read()\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    doc = ''\n",
    "    for para in soup.find_all('p'):\n",
    "        para = para.find_all(text=True)\n",
    "        text = ''\n",
    "        for line in para:\n",
    "            text = text+line+' '\n",
    "        doc = doc+text+' '\n",
    "    doc = doc.lower()\n",
    "    doc = doc.split(' ')\n",
    "    tokens = []\n",
    "    for word in doc:\n",
    "        word = makeVal(word)\n",
    "        if word not in stopwords.words('english') and len(word)>1 and word != 'html':\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFilePath(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSortedList(inv_index):\n",
    "    sorted_index = []\n",
    "    for word in inv_index:\n",
    "        value = str(tuple(inv_index[word]))\n",
    "        sorted_index.append(word + \" : \" + value)\n",
    "    sorted_index = sorted(sorted_index)\n",
    "    return sorted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create the list of unique words in the index\n",
    "def getUniqueList(inv_index):\n",
    "    num_uni_words = len(inv_index)\n",
    "    sorted_index = []\n",
    "    makeFilePath('Inverted Indexes/')\n",
    "    path = 'Inverted Indexes/Unique_Words.txt'\n",
    "    file = open(path,'w+')\n",
    "    file.write('Number of unique words: '+str(num_uni_words)+'\\n')\n",
    "    for word in inv_index:\n",
    "        sorted_index.append(word)\n",
    "    sorted_index = sorted(sorted_index)\n",
    "    for entry in sorted_index:\n",
    "        file.write(entry + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getInvertedIndex(inv_index,name):\n",
    "    makeFilePath('Inverted Indexes/')\n",
    "    path = 'Inverted Indexes/' + name + '.txt'\n",
    "    file = open(path,'w+')\n",
    "    sorted_index = getSortedList(inv_index)\n",
    "    for entry in sorted_index:\n",
    "        file.write(entry + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def freqIndex(folder):\n",
    "    #Create the index storage dictionary\n",
    "    inv_index = {}\n",
    "    doc_num = 0\n",
    "    #Read all the files in the crawled pages folder\n",
    "    path = folder + '/'\n",
    "    for filename in glob.glob(os.path.join(path,'*.html')):\n",
    "        doc_num += 1\n",
    "        #Create temp dictionary\n",
    "        d_freq = {}\n",
    "        #Convert the file to a list of tokens, or words\n",
    "        tokens = getTokens(filename)\n",
    "        for word in tokens:\n",
    "            if word not in d_freq:\n",
    "                #add the word to the dictionary as a key, with value 0\n",
    "                d_freq[word]=1\n",
    "            else:\n",
    "                #increment the value of the word in dictionary by 1\n",
    "                d_freq[word]+=1\n",
    "        #write the values of d_temp to inv_index\n",
    "        for word in d_freq:\n",
    "            value = [doc_num,d_freq[word]]\n",
    "            if word not in inv_index:\n",
    "                inv_index[word] = []\n",
    "                inv_index[word].append(value)\n",
    "            else:\n",
    "                inv_index[word].append(value)\n",
    "    #Write the inverted index to file\n",
    "    getInvertedIndex(inv_index,'Frequency Index')\n",
    "    getUniqueList(inv_index)\n",
    "    print('Frequency index created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createReport():\n",
    "    makeFilePath('Inverted Indexes/')\n",
    "    path = 'Inverted Indexes/' + 'Index_Report' + '.txt'\n",
    "    file = open(path,'w+')\n",
    "    file.write('This program works by reading the crawled web pages,'+'\\n')\n",
    "    file.write(' parsing the text using BeautifulSoup package, '+'\\n')\n",
    "    file.write('and tokenising it, creating a list of words. '+'\\n')\n",
    "    file.write('It then removes the stopwords using the nltk package. '+'\\n')\n",
    "    file.write('Afterword, it creates an index file of all the words '+'\\n')\n",
    "    file.write('and there frequencies. It also creates an index with '+'\\n')\n",
    "    file.write('the positions of the words within the documents.')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def posIndex(folder):\n",
    "    #Create the index storage dictionary\n",
    "    inv_index = {}\n",
    "    doc_num = 0\n",
    "    #Read all the files in the crawled pages folder\n",
    "    path = folder + '/'\n",
    "    for filename in glob.glob(os.path.join(path,'*.html')):\n",
    "        doc_num += 1\n",
    "        #Convert the file to a list of tokens, or words\n",
    "        tokens = getTokens(filename)\n",
    "        #Get the postion of the word in the file\n",
    "        word_pos = 0\n",
    "        for word in tokens:\n",
    "            value = str(tuple([doc_num,word_pos]))\n",
    "            if word not in inv_index:\n",
    "                #add the word to the dictionary as a key, with first position\n",
    "                inv_index[word]= []\n",
    "                inv_index[word].append(value)\n",
    "            else:\n",
    "                #add the current postion to index\n",
    "                inv_index[word].append(value)\n",
    "            #increment word position\n",
    "            word_pos += 1\n",
    "    #Write the inverted index to file\n",
    "    getInvertedIndex(inv_index,'Position Index')\n",
    "    print ('Position index created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency index created!\n",
      "Position index created!\n"
     ]
    }
   ],
   "source": [
    "freqIndex('Crawled_Pages')\n",
    "posIndex('Crawled_Pages')\n",
    "createReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
